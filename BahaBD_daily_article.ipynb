{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 目標: \n",
    "# 1. 爬巴哈姆特的遊戲排行資料\n",
    "# 2. 找出黑沙討論版的討論概況\n",
    "# source: https://pala.tw/python-web-crawler/\n",
    "\n",
    "import time\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen, Request\n",
    "import random\n",
    "import json    \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "\n",
    "# Google sheet AP\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先創造出資料需求的 list \n",
    "title_list = []\n",
    "gp_list = []\n",
    "reply_list = []\n",
    "click_list = []\n",
    "lastReply_list = []\n",
    "author_list = []\n",
    "posttime_list = []\n",
    "url_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找出該篇文章的 標題, GP, 回復, 點擊 , 最後編輯, 作者\n",
    "\n",
    "# tag 的邏輯\n",
    "pattern1 = re.compile(\"^b-list__main\")\n",
    "pattern2 = re.compile(r\"^b-list__summary__gp b-gp\")\n",
    "\n",
    "# 爬前五 rang() 頁的資料\n",
    "for i in range(5):\n",
    "    # 把網址讀入\n",
    "    url = \"https://forum.gamer.com.tw/B.php?page={}&bsn=19017\".format(i+1)\n",
    "    req = Request(url,headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    html = urlopen(req).read().decode(\"utf-8\")\n",
    "    soup=  BeautifulSoup(html, features='lxml')\n",
    "    \n",
    "    # 抓 title\n",
    "    # 當該討論串已無文章時,會有不同 tag 所以要拆開處理   \n",
    "    title_tag = soup.find_all(\"td\",{\"class\":pattern1})\n",
    "    for i in title_tag:\n",
    "        if i.find_all(\"a\") != []:\n",
    "            title_list.append(i.find(\"a\").contents[0])\n",
    "        else:\n",
    "            title_list.append(i.find(\"span\").contents[0])\n",
    "            \n",
    "\n",
    "    # 抓文章 Post 的時間   \n",
    "    # 此時的 post time 會忽略文章標題 == 本討論串已無文章\n",
    "    id_tag = soup.find_all(\"td\",{\"class\":\"b-list__summary\"})\n",
    "    for i in id_tag:\n",
    "        title_id = str(i.find(\"a\"))[9:-6]\n",
    "        url2 = \"https://forum.gamer.com.tw/C.php?bsn=19017&snA=\" + title_id\n",
    "        # 新增文章連結\n",
    "        url_list.append(url2)\n",
    "        \n",
    "        req2 = Request(url2,headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        html2 = urlopen(req2).read().decode(\"utf-8\")\n",
    "        soup2 =  BeautifulSoup(html2, features='lxml')\n",
    "        post_tag = soup2.find_all(\"div\",{\"class\":\"c-post__header__info\"},limit=1)\n",
    "    \n",
    "        for j in post_tag:\n",
    "            temp = str(j).replace(\"data-mtime=\",\"/\").replace(\" href=\",\" /\")\n",
    "            posttime_list.append(temp.split('\" /\"')[1])\n",
    "            \n",
    "\n",
    "    # 抓 gp\n",
    "    td_tag = soup.find_all( \"td\")\n",
    "    count = 1\n",
    "    for i, tag in enumerate(td_tag,0):\n",
    "        if i >4:\n",
    "            # 如果標籤裡面有 xxx gp 的話就印出 gp 的值\n",
    "            # count 邏輯是依實際 html tag 來歸納\n",
    "            if i - 4 * count == 1: \n",
    "                # GP 有值的話就給值 沒有的話就給 0\n",
    "                if tag.find_all(\"span\", {\"class\":pattern2}) != []:\n",
    "                    gp_list.append(int(tag.find(\"span\", {\"class\":pattern2}).contents[0]))\n",
    "                else:\n",
    "                    gp_list.append(0)\n",
    "                count = count +1\n",
    "    \n",
    "    ## 抓 reply & click\n",
    "    count_tag = soup.find_all(\"p\",{\"class\":\"b-list__count__number\"})\n",
    "    for a in count_tag:\n",
    "        # 把 <span> </span> 透過 str slice 拿掉\n",
    "        reply_list.append(str(a.contents[1])[6:-7])\n",
    "        click_list.append(str(a.contents[3])[6:-7])\n",
    "        \n",
    "        \n",
    "    ## 抓 最後回覆時間\n",
    "    # 先找 b-list_time 再找 a\n",
    "    time_tag = soup.find_all(\"p\",{\"class\":\"b-list__time__edittime\"})\n",
    "    for i in time_tag:\n",
    "        lastReply_list.append(i.find(\"a\").contents[0])\n",
    "        \n",
    "    # 找PO 文者\n",
    "    author_tag = soup.find_all(\"p\",{\"class\":\"b-list__count__user\"})\n",
    "    for i in author_tag:\n",
    "        author_list.append(i.find(\"a\").contents[0])\n",
    "        \n",
    "# 透過 title_list 中的值來調整 post_list \n",
    "# 若 title_list 值為 \"本討論串已無文章 則調整\n",
    "for i,title in enumerate(title_list):\n",
    "    if title ==\"本討論串已無文章\":\n",
    "        posttime_list.insert(i,\"NA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "150\n",
      "150\n",
      "150\n",
      "150\n",
      "150\n",
      "150\n"
     ]
    }
   ],
   "source": [
    "# 確認資料長度相同\n",
    "### 之前抓 100 頁時, 有遇到 長度不依的情況, 不知道如何排除\n",
    "print(len(title_list))\n",
    "print(len(gp_list))\n",
    "print(len(reply_list))\n",
    "print(len(click_list))\n",
    "print(len(lastReply_list))\n",
    "print(len(author_list))\n",
    "print(len(posttime_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 轉 dataframe\n",
    "\n",
    "#調整小數位\n",
    "pd.set_option('precision', 2)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df[\"title\"] = title_list\n",
    "df[\"author\"] = author_list\n",
    "df[\"posttime\"] = posttime_list\n",
    "df[\"last_reply\"] = lastReply_list\n",
    "\n",
    "df[\"click\"] = click_list\n",
    "df.click = df.click.astype(\"int64\")\n",
    "\n",
    "df[\"reply\"] = reply_list\n",
    "df.reply = df.reply.astype(\"int64\")\n",
    "\n",
    "df[\"gp\"] = gp_list\n",
    "\n",
    "\n",
    "# 新增 \"type\" 欄位\n",
    "def type_list(word):\n",
    "    if word[0] != \"【\":\n",
    "        return \"NA\"\n",
    "    else:\n",
    "        return word[1:3]\n",
    "    \n",
    "df[\"type\"] = df.title.apply(type_list)\n",
    "\n",
    "\n",
    "# 調整 last_edit 的時間欄位\n",
    "today = time.strftime(\"%m/%d\")\n",
    "temp = dt.datetime.now() - dt.timedelta(days=1)\n",
    "yesterday = temp.strftime('%m/%d')\n",
    "df[\"last_reply\"] = df[\"last_reply\"].replace(\"今日\" , today, regex=True).replace(\"昨日\" , yesterday, regex=True)\n",
    "\n",
    "# 調整發文時間欄位的型態\n",
    "df[\"posttime\"] = pd.to_datetime(df[\"posttime\"],format = \"%Y-%m-%d %H:%M:%S\",errors= \"coerce\")\n",
    "\n",
    "# timedelta\n",
    "df[\"times_AF_post\"] = pd.to_timedelta(dt.datetime.now() - df[\"posttime\"],errors = \"coerce\", unit=\"d\")\n",
    "# change to float through / timedelta\n",
    "df[\"days_AF_post\"] = (pd.to_timedelta(dt.datetime.now() - df[\"posttime\"],errors = \"coerce\", unit=\"d\"))/dt.timedelta (days=1)\n",
    "\n",
    "\n",
    "\n",
    "# 找出文章重要性的指標\n",
    "# 每天平均的點擊為 # 每天平均地回復為\n",
    "# 若文章天數小於1 則用1 來除\n",
    "def avg_reply(df):\n",
    "    if df[\"days_AF_post\"] <1:\n",
    "        return round(df[\"reply\"],2)\n",
    "    else:\n",
    "        return round(df[\"reply\"]/df[\"days_AF_post\"],2)\n",
    "    \n",
    "df[\"avg_reply\"] = df.apply(avg_reply, axis =1)\n",
    "\n",
    "\n",
    "def avg_click(df):\n",
    "    if df[\"days_AF_post\"] <1:\n",
    "        return round(df[\"click\"],2)\n",
    "    else:\n",
    "        return round(df[\"click\"]/df[\"days_AF_post\"],2)\n",
    "    \n",
    "df[\"avg_click\"] = df.apply(avg_click, axis =1)\n",
    "\n",
    "# 加入 URL\n",
    "df[\"url\"] = url_list\n",
    "\n",
    "# 把 本討論串已無文章 的資料拿掉\n",
    "df = df[df[\"title\"] != \"本討論串已無文章\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 輸出檔案 給 Brian (找近兩天PO 文的資料)\n",
    "# 目的: 減少去巴哈黑沙版找出今日PO文的時間\n",
    "# 篩選兩天內的 po 文\n",
    "temp_df= df.iloc[np.where(df[\"times_AF_post\"]  < dt.timedelta(days=2))]\n",
    "output = temp_df.sort_values(\"click\",ascending=False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use creds to create a client to interact with the Google Drive API\n",
    "scope = ['https://spreadsheets.google.com/feeds',\n",
    "         'https://www.googleapis.com/auth/drive']\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_name('client_id.json', scope)\n",
    "client = gspread.authorize(creds)\n",
    " \n",
    "# Find a workbook by name and open the first sheet\n",
    "# Make sure you use the right name here.\n",
    "sheet = client.open(\"daily_article\").sheet1\n",
    "\n",
    "# 把爬到的值填入 google sheet\n",
    "t = time.strftime(\"%Y-%m-%d-%H-%M\",time.localtime())\n",
    "sheet.update_cell(1,2,t)\n",
    "\n",
    "\n",
    "for i in range(len(output)):\n",
    "    # 要新增欄位的位置參數\n",
    "    cell_list = sheet.range('A{}:M{}'.format(i+3,i+3))\n",
    "    \n",
    "    # 要新增的值, 每一格每一新增\n",
    "    j =0\n",
    "    for cell in cell_list:\n",
    "        cell.value = str(output.iloc[i][j+1])\n",
    "        j = j+1\n",
    "    \n",
    "    # 新增\n",
    "    sheet.update_cells(cell_list)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
